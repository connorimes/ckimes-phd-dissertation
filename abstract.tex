\begin{abstract}

This dissertation addresses challenges in balancing performance and power/energy consumption in computing systems.
In many cases, applications do not require all of a system's resources in order to achieve desired behavior, \eg an application performance goal or reasonable system energy consumption.
Modern systems expose knobs for tuning resources, like processor frequency or core allocation, that have a quantifiable impact on application performance and system power consumption.
The result is a tradeoff space that can be navigated by resource schedulers to achieve desired behavior, sacrificing one dimension in favor of another, \eg increased performance at the cost of increased power or energy consumption.
The optimal knob settings required to achieve desired behavior depend on both the application and system, even changing during the course of execution as applications progress through different processing phases.
The challenge in designing general and portable solutions to these problems arises from the diversity in both hardware and software systems.

We first address the problem of meeting application performance goals while minimizing energy consumption with two projects---POET and CoPPer.
Both projects use control theory, which provides strong guarantees about convergence to the goal and stability in the face of dynamic behavior.
In contrast, commonly used heuristic techniques cannot provide these guarantees and are also not always portable.
POET is a general solution that is portable between applications and systems, as well as independent of different knob types and their allowable settings.
POET produces resource schedules to exactly meet performance goals while achieving optimal energy consumption.
CoPPer leverages recent power capping technology in place of software-managed dynamic voltage and frequency scaling (DVFS), which is being deprecated by hardware vendors.
CoPPer overcomes challenges presented by the non-linear relationship between performance and power to meet performance goals while leaving the energy optimization to hardware, which responds more rapidly to changes in application resource requirements than software.

We then address the problem of optimizing energy efficiency, thus reducing the cost of running applications.
Machine learning classifiers, driven by low-level hardware performance counters, predict the most energy-efficient knob settings based on current application resource utilization.
Importantly, no application modifications are necessary.
We evaluate this approach in the High Performance Computing (HPC) domain, more aggressively trading performance for energy savings than has historically been done, reducing the cost of scientific insight.
Extrapolating from empirical single-node performance and power results indicates that scaling the solution to hardware over-provisioned, power-constrained clusters could actually increase total cluster throughput by up to 40\%.

Unlike commonly-used static resource scheduling techniques, our three projects dynamically adapt to changing application and system behavior at runtime to provide more optimal results.
Furthermore, their designs are independent of particular applications and systems, making them portable to a wide range of computing platforms.

\end{abstract}
