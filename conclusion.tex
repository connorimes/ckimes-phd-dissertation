\chapter{Conclusion}

This dissertation addresses balancing application performance and system energy consumption in computing systems.
The first two projects, POET and CoPPer, are adaptive control-theoretic approaches for meeting soft performance constraints which simultaneously minimize energy consumption.
The third project uses machine learning classification techniques, driven by hardware performance counters, to predict energy-efficient system settings at runtime to optimize the performance/energy tradeoff.
Unlike static resource allocation techniques, our solutions dynamically adjust resource allocations based on runtime feedback to adapt to changing application and system behavior.

Our early work demonstrates that heuristic approaches for meeting timing constraints and reducing energy consumption are not portable between applications and systems, \eg the classic \emph{race-to-idle} heuristic can be completely the wrong approach to use.
We therefore propose POET, a portable approach to resource management that functions independent of platform timing/energy tradeoffs or particular system knobs.
POET uses control theory to meet soft performance (or latency) goals and mathematical optimization to minimize energy consumption.
We demonstrate POET's ability to meet soft performance constraints and achieve near-optimal energy consumption on two very different embedded systems---a Vaio tablet with an Intel Haswell processor, and an ODROID-XU+E with a heterogeneous ARM big.LITTLE SoC.
We then demonstrate POET's effectiveness on a dual-socket server-class system with an order of magnitude more knob settings and significantly more compute power than the embedded platforms.
In addition, we briefly discuss Bard, a follow-on project which modifies POET to allow switching between performance or power goals while optimizing energy or performance, respectively.
% Furthermore, POET has been used as the foundation for other researchers' projects.

Software management of DVFS is becoming deprecated, necessitating a replacement for the abundance of controllers that previously relied on DVFS to meet performance constraints.
We respond to this demand with CoPPer, which also uses adaptive control-theoretic techniques to meet soft performance goals while optimizing energy.
CoPPer actuates hardware power caps instead of DVFS, but leaves the optimization to the hardware, which can adapt to changing resource demands more quickly than software.
CoPPer addresses new challenges that arise from power capping, without requiring an explicit behavior model.
We evaluate CoPPer on a dual-socket system using Intel RAPL, demonstrate that it meets soft performance constraints, and outperforms a state-of-the-art DVFS controller (a modified version of POET) in achieving low energy consumption.
Additionally, CoPPer introduces a novel \emph{gain limit} to prevent over-allocating power when it is not beneficial, \eg for memory-bound applications or unachievable performance goals.

Finally, we shift focus to the High Performance Computing (HPC) domain and propose optimizing energy efficiency to minimize the runtime cost of computation.
Beyond the obvious energy/cost savings achieved from reducing energy, we extrapolate from our single-system results to show that optimizing energy efficiency in hardware over-provisioned, power-constrained clusters (as we expect future exascale HPC systems to be) increases the total throughput of the cluster to complete more science for the same runtime costs.
To avoid modifying applications, we map low-level hardware performance counters (features) to energy-efficient system knob settings.
We propose using machine learning classification techniques to predict energy-efficient settings in-situ on compute nodes, since they are low-overhead compared to existing estimation approaches used in HPC resource management.
The complexity of the feature space mappings makes simple classification approaches insufficient for solving this resource allocation problem.
We evaluate five of the most promising classifiers and demonstrate that they dramatically reduce the energy consumption of production HPC genome assembly applications on a large shared-memory compute node with low overhead and robustness to the availability of different performance counters.


\section{Future Work}

While this dissertation is on performance and power/energy consumption, there are other tradeoffs available in computing systems.
For example, tuning application accuracy is an ongoing area of research.
A resource allocation scheme could dynamically choose from a set of functions that offer different performance and computation precision, or a configurable hardware component can increase performance or reduce energy consumption at the expense of less accurate results.
Computation accuracy could be a goal or optimization in any of the three major projects presented in this dissertation and would be useful on systems ranging from embedded platforms to HPC clusters.
It is a particularly challenging area of research because different applications have different concepts of what it means to be accurate.
Accuracy can also be difficult to quantify, especially when such values are dependent on human perception, \eg for various types of multimedia.
Furthermore, the range of acceptable accuracies is context-dependent, \eg by application and/or deployment scenario.

Our control-theoretic projects support meeting a single constraint and optimizing in another dimension.
However, software must often manage multiple constraints, \eg meet a performance goal but also respect a power cap.
Combining multiple controllers is a logical next step, but they must be coordinated so as not to conflict with each other.
In simple cases, controllers tune knobs that do not effect each other, in which case little or no direct coordination is needed, \eg one controller tunes application-level knobs while another tunes system knobs.
Otherwise the controllers must either communicate directly or be configured such that they can still adapt correctly, \eg by using different pole values and/or window periods.
In either case, they should not be tuning the same knobs, otherwise their internal state would be inaccurate---they would fail to meet their constraints and any optimization they attempt to provide could suffer as well.

Our machine learning classification framework currently optimizes energy efficiency on individual compute nodes for single-node jobs.
To actually maximize the throughput of hardware over-provisioned, power-constrained clusters, the solution must be adapted to scale out to multiple nodes.
Naively deploying the current solution on all nodes in a cluster will not achieve this, and could result in less-than-optimal energy efficiency and violations of the global power constraint.
First, a component to schedule jobs while respecting the cluster power constraint is needed, likely in coordination with accurate predictions of how much power individual nodes require while running their assigned jobs energy-efficiently (keeping in mind that the power consumption fluctuates as applications move through phases).
Additionally, tighter coordination between nodes is needed to support jobs that scale across multiple nodes, so that one node does not slow down the rest, resulting in reduced total energy efficiency.
% Our machine learning classification framework optimizes energy efficiency, but can be easily adapted to support different goals derived from performance counters.

Availability of runtime feedback from which to make informed decisions is a fundamental requirement of self-aware systems.
Applications and users want to specify contextual goals, \eg frames processed in a video encoder, not low-level metrics like instructions retired.
Hardware performance counters are the de facto standard today, but do not provide high-level metrics like a true measure of application progress.
Few such mechanisms are available today, and typically require modifying applications to provide appropriate instrumentation, \eg using Heartbeats.
Language constructs or operating system interfaces could provide more portable approaches for applications to report metrics.
Some measurements could potentially be inferred from the software design, or at least only require a developer to specify a few attributes in their code without linking to additional libraries.
A self-aware system can then rely on these portable metrics rather than requiring the developer to provide instrumentation.
