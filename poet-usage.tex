\section{Experimental Setup}
\label{sec:poet-usage}

This section details the applications used to evaluate POET and the different evaluation platforms.


\subsection{Applications}

To represent a wide variety of embedded applications, we use eight different benchmarks, none of which were originally written to provide predictable timing.
We choose applications that do not enforce any timing guarantees to challenge POET's approach as much as possible.

The first five applications are included in the PARSEC benchmark suite~\cite{parsec}.
Specifically, we use \app{blackscholes}, \app{bodytrack}, \app{facesim}, \app{ferret}, and \app{x264}.
Bodytrack and x264 process video input and could be required to match the frame rate of a live feed (\eg from an on-board camera).
Ferret is a toolkit for content-based similarity search of non-text data and should satisfy a latency requirement on how fast results are returned to users.
Facesim creates realistic animations of a human face from a model and time sequence of muscle movements, and must maintain a real-time frame rate.
The sixth and seventh applications are \app{dijkstra} and \app{sha}, from the ParMiBench benchmark suite~\cite{parmibench}.
Dijkstra\footnote{We use the parallelized multiple queue implementation provided with the ParMiBench benchmark suite.} computes single-source shortest paths in a graph.
SHA (Secure Hash Algorithm) is used for secure storage and transmission of data and must maintain response time to ensure timely communication.
The last application is \app{STREAM}~\cite{stream}, a synthetic benchmark for measuring sustainable memory bandwidth, representing a variety of memory-bound applications.

All benchmarks are modified as discussed in~\secref{poet-implementation}, adding Heartbeats and POET calls.
All application inputs used are packaged with the original benchmarks, with the exception of the x264 input which comes from a set of standard test sequences.
Recall that these applications were not originally designed to provide predictable timing.


\subsection{Evaluation Platforms}

The evaluation is broken down into two categories.
In the first part of the evaluation, we use two modern embedded devices with different hardware---a Sony VAIO SVT11226CXB tablet PC with a mobile Intel Haswell processor, and a Hardkernel ODROID-XU+E ARM big.LITTLE development platform.
We selected these two platforms because prior work has shown that they expose different timing and energy tradeoffs~\cite{Imes2014}.
% \tblref{poet-embedded-systemknobs} shows the hardware details for both, highlighting the configurable resources, the cardinality of the set of alternatives for each, and the maximum speedup achievable by manipulating that resource alone.
Both platforms run Ubuntu Linux 14.04 LTS.
The Vaio uses kernel 3.13.0, while the ODROID runs kernel 3.4.104.

% \begin{table}[t]
% \caption{Embedded system configurations.}
% \centering
% \begin{tabular}{clcc}
%   & \textbf{Resource} & \textbf{Settings} & \textbf{Max Speedup} \\
% \hline
% \hline
%   \multirow{3}{*}{\begin{turn}{90}\textbf{Vaio}\end{turn}} 
%   & cores        &  2 & 1.81 \\
%   & core speeds  & 11 & 2.72 \\
%   & hyperthreads &  2 & 1.10 \\ 
% \hline
% \hline
%   \multirow{4}{*}{\begin{turn}{90}\textbf{O-XU+E}\end{turn}} 
%   & big cores          & 4 & 6.10 \\
%   & big core speeds    & 9 & 1.97 \\
%   & LITTLE cores       & 4 & 3.94 \\
%   & LITTLE core speeds & 8 & 2.40 \\
% \hline
% \hline
% \end{tabular}
% \label{tbl:poet-embedded-systemknobs}
% \end{table}

The second part of the evaluation uses a dual-socket server system, where each socket contains 8 cores.
With hyperthreading, the system exposes 32 virtual cores.
There are 16 DVFS settings available, including TurboBoost.
The server system also runs Ubuntu 14.04 LTS with Linux kernel 3.13.0.

A \textbf{configuration} is a unique combination of allowable values for the system resources.
In all cases, \app{cpufrequtils} controls processor clock speeds and \app{taskset} controls core allocation.
While the Vaio claims to support different frequency settings on different virtual cores, our experience leads us to conclude that this is not the case.
Thus, we allow only configurations where all cores are set to the same frequency.
The ODROID's version of the Exynos5 Octa does not support executing on the big and LITTLE clusters simultaneously, and all cores in a cluster must operate at the same frequency.
We use a mainline Linux kernel with the default In Kernel Switcher for managing cluster migration.
%IKS maintains compatibility with existing schedulers by modifying existing DVFS systems, but at a cost of supporting execution on only one cluster at a time. This is achieved on the ODROID by using dummy frequency values for the LITTLE cores so as not to overlap with the frequencies provided by the big cores.
On the server system, we set the DVFS frequency on all cores, regardless if a particular core is being used.

\begin{table}[t]
% \vskip -1.5em
\centering
\caption{System power characteristics.}
\begin{tabular}{cccc}
  \textbf{System} & \textbf{Idle Power} & \textbf{Min Power} & \textbf{Max Power} \\
  \hline
  \hline
  Vaio        & 2.50 W  & 3.04 W  & 8.05 W \\
  ODROID-XU+E & 0.12 W  & 0.17 W  & 8.14 W \\
  Server      & 17.90 W & 37.80 W & 199.26 W \\
  % ODROID-XU3  & 0.21 W  & 0.19 W  & 6.37 W \\
  \hline
  \hline
\end{tabular}
\label{tbl:poet-power}
\end{table}

To capture power measurements on the Vaio and the server system, we use the Model-Specific Register (MSR) of the Haswell processor~\cite{SandyBridge}.
On the ODROIDs, we poll INA-231 power sensors~\cite{ina231} to capture power data for the A15 and A7 clusters as well as for the DRAM and for the GPU.
Basic power figures of the three platforms are shown in~\tblref{poet-power}.
Capturing these metrics naturally requires hardware resources that expose power or energy data to software.
The modified version of Heartbeats includes energy readers for some common hardware (\eg the MSR) and exposes a simple interface for extending to new hardware.
Collecting power data on new platforms with different power or energy monitors is easy and does not require any modifications to POET.

\begin{table}[t]
\small
\centering
\caption{Embedded systems evaluation inputs and configuration details.}
\begin{tabular}{cccc}
  \textbf{Application} & \textbf{Input} & \textbf{Jobs} & \textbf{Window Size} \\
  \hline
  \hline
  blackscholes   & 1 million options              & 400 batches   & 20 \\
  bodytrack      & sequenceB                      & 261 frames    & 20 \\
  facesim        & Storytelling                   & 100 frames    & 20 \\
  ferret         & corel:lsh                      & 2,000 queries & 20 \\
  x264           & ducks\_take\_off               & 500 frames    & 20 \\
  dijkstra       & input\_small                   & 1,000 paths   & 20 \\
  sha            & in\_file(1-16)                 & 1,000 hashes  & 50 \\
  STREAM         & self-generated                 & 1,000 updates & 50 \\
  \hline
  \hline
\end{tabular}
\label{tbl:poet-embedded-inputs}
\end{table}

\begin{figure}[t]
  \centering
  \input{img/poet/variability-embedded.tex}
  \caption{Embedded systems application latency variability.}
  \label{fig:poet-embedded-variation}
\end{figure}

\tblref{poet-embedded-inputs} shows the inputs used for each of the applications on these embedded platforms.
We quantify this inherent unpredictability by measuring the latency of each job and computing the standard deviation and mean over all jobs in an application.
\figref{poet-embedded-variation} shows the ratio of standard deviation to mean for each application when running without POET.
The figure shows that our applications have a range of natural behavior from low variance (implying natural predictability, \eg blackscholes) to high variance (meaning that the application naturally has widely distributed latencies, \eg x264).
The variability in the applications is largely the same across platforms, indicating that it is a fundamental property of the applications and not the devices.

\begin{table}[t]
\small
\centering
\caption{Server-class system inputs and configuration details.}
\begin{tabular}{cccc}
  \textbf{Application} & \textbf{Input} & \textbf{Jobs} & \textbf{Window Size} \\
  \hline
  \hline
  blackscholes   & 10 million options             & 400 batches   & 50 \\
  bodytrack      & sequenceB                      & 261 frames    & 50 \\
  facesim        & Storytelling                   & 100 frames    & 20 \\
  ferret         & corel:lsh                      & 2,000 queries & 50 \\
  x264           & rush\_hour                     & 1,500 frames  & 100 \\
  dijkstra       & input\_large                   & 1,000 paths   & 50 \\
  sha            & in\_file(1-16)                 & 1,000 hashes  & 50 \\
  STREAM         & self-generated                 & 1,000 updates & 50 \\
  \hline
  \hline
\end{tabular}
\label{tbl:poet-server-inputs}
\end{table}

\begin{figure}[t]
  \input{img/poet/variability-server.tex}
  \caption{Server system application latency variability.}
  \label{fig:poet-server-variation}
\end{figure}

\tblref{poet-server-inputs} lists the inputs used for each of the applications on the server system.
The server-class system is significantly more powerful than the embedded systems.
The overhead of changing resource allocations is also higher due to the larger core count.
As a result, we increased both the size or length of some inputs and the window period size.
Again, we quantify the variability in the applications and present the results in \figref{poet-server-variation} and find the results are similar to those from the embedded systems.
